From 43326e565071f4f95c802662d9a975a2d754d529 Mon Sep 17 00:00:00 2001
From: Harm Hanemaaijer <fgenfb@yahoo.com>
Date: Mon, 24 Jun 2013 14:41:10 +0200
Subject: [PATCH] Optimize kernel memcpy/memset functions for ARM v6/v7

Optimize prefetch for modern ARM platforms in memcpy variants by
prefetching at 32-byte or 64-byte aligned addresses, depending on
L1_CACHE_BYTES. Additionally, the CALGN macro optional code is
enabled for the v6 and v7 ARM architectures to force write
alignment to 32 bytes in the memcpy variants (16-byte write
alignment is also possible). For regular memcpy (as opposed to
copy_from_user or copy_to_user), the alignment and tail code is
optimized, using multiple word instead of single word loads and
stores. Write alignment is disabled for copy_from_user and
copy_to_user. There is also a new fast path for the common case of
smaller word aligned sizes, which reduces overhead and speeds up
small requests.

For memzero and memset, write alignment is enabled for armv6 with
a seperate macro CALGN_MEMSET that regulates whether alignment
code is emitted in the memset and memzero assembler functions.
MEMSET_WRITE_ALIGNMENT_BYTES sets the number of bytes to align
to (8 for armv6). For armv7, write alignment did not show benefit
in testing for memzero/memset.

The function copy_page is also optimized with L1_CACHE_BYTES
aligned prefetches.

A new constant, PREFETCH_DISTANCE, is defined in
arch/arm/include/asm/cache.h which when multiplied with
L1_CACHE_BYTES is equal to the preload offset used for
prefetching. It defaults to 3 (96) for armv6 (L1_CACHE_BYTES ==
32), and 3 (192) for armv7 (L1_CACHE_BYTES == 64).

As a side-effect, memset now properly returns the original
destination address in r0. This may fix issues compiling ARM
kernels with gcc versions greater than 4.7. Kernels greater
than 3.9 may already contain a fix for this (which has been
merged into this patch).

Signed-off-by: Harm Hanemaaijer <fgenfb@yahoo.com>
---
 arch/arm/include/asm/assembler.h |   24 +-
 arch/arm/include/asm/cache.h     |   12 +
 arch/arm/lib/copy_from_user.S    |   35 ++-
 arch/arm/lib/copy_page.S         |   64 +++--
 arch/arm/lib/copy_template.S     |  538 ++++++++++++++++++++++++++++++++++----
 arch/arm/lib/copy_to_user.S      |   35 ++-
 arch/arm/lib/memcpy.S            |   18 +-
 arch/arm/lib/memset.S            |   78 +++---
 arch/arm/lib/memzero.S           |   97 ++++---
 9 files changed, 748 insertions(+), 153 deletions(-)

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 05ee9ee..8f8f8e3 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -58,10 +58,13 @@
  */
 #if __LINUX_ARM_ARCH__ >= 5
 #define PLD(code...)	code
+#define NO_PLD(code...)
 #else
 #define PLD(code...)
+#define NO_PLD(code...) code
 #endif
 
+
 /*
  * This can be used to enable code to cacheline align the destination
  * pointer when bulk writing to memory.  Experiments on StrongARM and
@@ -70,14 +73,33 @@
  * is used).
  *
  * On Feroceon there is much to gain however, regardless of cache mode.
+ * The armv6 architecture benefits from write alignment to a 32-byte
+ * boundary. On armv7, write alignment to a 32-byte boundary increases
+ * performance for regular memcpy so it is enabled.
  */
-#ifdef CONFIG_CPU_FEROCEON
+#if defined(CONFIG_CPU_FEROCEON) || __LINUX_ARM_ARCH__ >= 6
 #define CALGN(code...) code
+#define WRITE_ALIGN_BYTES 32
 #else
 #define CALGN(code...)
 #endif
 
 /*
+ * Write alignment for memset/memzero is enabled for armv6.
+ */
+
+#if defined(CONFIG_CPU_FEROCEON) || __LINUX_ARM_ARCH__ == 6
+#define CALGN_MEMSET(code...) code
+#if defined(CONFIG_CPU_FEROCEON)
+#define MEMSET_WRITE_ALIGN_BYTES 32
+#else
+#define MEMSET_WRITE_ALIGN_BYTES 8
+#endif
+#else
+#define CALGN_MEMSET(code...)
+#endif
+
+/*
  * Enable and disable interrupts
  */
 #if __LINUX_ARM_ARCH__ >= 6
diff --git a/arch/arm/include/asm/cache.h b/arch/arm/include/asm/cache.h
index 75fe66b..01e31e0 100644
--- a/arch/arm/include/asm/cache.h
+++ b/arch/arm/include/asm/cache.h
@@ -8,6 +8,18 @@
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 /*
+ * Set the prefetch distance in units of L1_CACHE_BYTES based on the
+ * cache line size. The prefetch distance is used by the memcpy
+ * variants and copy_page.
+ */
+
+#if L1_CACHE_BYTES == 64
+#define PREFETCH_DISTANCE 3
+#else
+#define PREFETCH_DISTANCE 3
+#endif
+
+/*
  * Memory returned by kmalloc() may be used for DMA, so we must make
  * sure that all such allocations are cache aligned. Otherwise,
  * unrelated code may cause parts of the buffer to be read into the
diff --git a/arch/arm/lib/copy_from_user.S b/arch/arm/lib/copy_from_user.S
index 66a477a..9bdd4fa 100644
--- a/arch/arm/lib/copy_from_user.S
+++ b/arch/arm/lib/copy_from_user.S
@@ -12,6 +12,7 @@
 
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#include <asm/cache.h>
 
 /*
  * Prototype:
@@ -39,6 +40,7 @@
 #define LDR1W_SHIFT	1
 #endif
 #define STR1W_SHIFT	0
+#define COPY_FUNCTION_FROM_USER
 
 	.macro ldr1w ptr reg abort
 	ldrusr	\reg, \ptr, 4, abort=\abort
@@ -64,6 +66,10 @@
 	W(str) \reg, [\ptr], #4
 	.endm
 
+	.macro str4w ptr reg1 reg2 reg3 reg4 abort
+	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4}
+	.endm
+
 	.macro str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
 	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4, \reg5, \reg6, \reg7, \reg8}
 	.endm
@@ -72,17 +78,21 @@
 	str\cond\()b \reg, [\ptr], #1
 	.endm
 
-	.macro enter reg1 reg2
-	mov	r3, #0
-	stmdb	sp!, {r0, r2, r3, \reg1, \reg2}
+	.macro enter_no_regs
+	/* Push the destination and the number of bytes. */
+	stmdb	sp!, {r0, r2}
 	.endm
 
-	.macro exit reg1 reg2
+	.macro exit_no_regs
+	/* Pop the original destination and number of bytes. */
 	add	sp, sp, #8
-	ldmfd	sp!, {r0, \reg1, \reg2}
+	/* Return 0 in r0. */
+	mov	r0, #0
+	mov	pc, lr
 	.endm
 
 	.text
+	.p2align 5
 
 ENTRY(__copy_from_user)
 
@@ -93,12 +103,21 @@ ENDPROC(__copy_from_user)
 	.pushsection .fixup,"ax"
 	.align 0
 	copy_abort_preamble
+	/*
+	 * Pop the original destination and the number of bytes
+         * off the stack.
+         */
 	ldmfd	sp!, {r1, r2}
+	/* Calculate the number of bytes not copied. */
 	sub	r3, r0, r1
+	mov	r5, lr
 	rsb	r1, r3, r2
-	str	r1, [sp]
+	/* Preserve the number of bytes not copied. */
+	mov	r4, r1
+	/* Call memzero(dest + bytes copied, bytes_not_copied). */
 	bl	__memzero
-	ldr	r0, [sp], #4
-	copy_abort_end
+	/* Return the number of bytes not copied in r0. */
+	mov	r0, r4
+	mov	pc, r5
 	.popsection
 
diff --git a/arch/arm/lib/copy_page.S b/arch/arm/lib/copy_page.S
index 6ee2f67..74bfac1 100644
--- a/arch/arm/lib/copy_page.S
+++ b/arch/arm/lib/copy_page.S
@@ -14,10 +14,22 @@
 #include <asm/asm-offsets.h>
 #include <asm/cache.h>
 
-#define COPY_COUNT (PAGE_SZ / (2 * L1_CACHE_BYTES) PLD( -1 ))
+/*
+ * Notes for Raspberry Pi:
+ * RPi does not like paired preloads in a 64-byte loop. Instead,
+ * use a 32-byte loop with one preload per loop. In addition,
+ * make sure no prefetching happens beyond the source region.
+ * The prefetch distance is set to 3 (96 bytes).
+ *
+ * This version should also be usable on architectures other than
+ * armv6 with properly defined L1_CACHE_BYTES and PREFETCH_DISTANCE
+ * (e.g. armv7 with L1_CACHE_BYTES == 64).
+ */
+
+#define COPY_COUNT (PAGE_SZ / (L1_CACHE_BYTES))
 
 		.text
-		.align	5
+		.p2align	5
 /*
  * StrongARM optimised copy_page routine
  * now 1.78bytes/cycle, was 1.60 bytes/cycle (50MHz bus -> 89MB/s)
@@ -25,23 +37,41 @@
  * the core clock switching.
  */
 ENTRY(copy_page)
-		stmfd	sp!, {r4, lr}			@	2
+		stmfd	sp!, {r4-r8, lr}		@	2
 	PLD(	pld	[r1, #0]		)
 	PLD(	pld	[r1, #L1_CACHE_BYTES]		)
+.if PREFETCH_DISTANCE > 2
+	PLD(	pld	[r1, #2 * L1_CACHE_BYTES]	)
+.if PREFETCH_DISTANCE > 3
+	PLD(	pld	[r1, #3 * L1_CACHE_BYTES]	)
+.if PREFETCH_DISTANCE > 4
+	PLD(	pld	[r1, #4 * L1_CACHE_BYTES]	)
+.endif
+.endif
+.endif
 		mov	r2, #COPY_COUNT			@	1
-		ldmia	r1!, {r3, r4, ip, lr}		@	4+1
-1:	PLD(	pld	[r1, #2 * L1_CACHE_BYTES])
-	PLD(	pld	[r1, #3 * L1_CACHE_BYTES])
+1:	PLD(	pld	[r1, #PREFETCH_DISTANCE * L1_CACHE_BYTES])
 2:
-	.rept	(2 * L1_CACHE_BYTES / 16 - 1)
-		stmia	r0!, {r3, r4, ip, lr}		@	4
-		ldmia	r1!, {r3, r4, ip, lr}		@	4
-	.endr
-		subs	r2, r2, #1			@	1
-		stmia	r0!, {r3, r4, ip, lr}		@	4
-		ldmgtia	r1!, {r3, r4, ip, lr}		@	4
-		bgt	1b				@	1
-	PLD(	ldmeqia r1!, {r3, r4, ip, lr}	)
-	PLD(	beq	2b			)
-		ldmfd	sp!, {r4, pc}			@	3
+.if L1_CACHE_BYTES == 32
+		ldmia	r1!, {r3-r6}			@	4+1
+	PLD(	sub	r2, r2, #1		)	@	1
+	NO_PLD(	subs	r2, r2, #1		)	@	1
+		ldmia   r1!, {r7, r8, ip, lr}
+		stmia	r0!, {r3-r6}			@	4
+	PLD(	cmp	r2, #PREFETCH_DISTANCE	)
+		stmia   r0!, {r7, r8, ip, lr}
+.else /* L1_CACHE_BYTES == 64 */
+		ldmia   r1!, {r3-r8, ip, lr}
+	PLD(	sub	r2, r2, #1		)	@	1
+	NO_PLD(	subs	r2, r2, #1		)	@	1
+		stmia	r0!, {r3-r8, ip, lr}		@	4
+		ldmia   r1!, {r3-r8, ip, lr}
+	PLD(	cmp	r2, #PREFETCH_DISTANCE	)
+		stmia	r0!, {r3-r8, ip, lr}		@	4
+.endif
+	PLD(	bgt	1b			)	@	1
+	NO_PLD(	bne	1b			)
+	PLD(	teq	r2, #0			)
+	PLD(	bne	2b			)
+		ldmfd	sp!, {r4-r8, pc}		@	3
 ENDPROC(copy_page)
diff --git a/arch/arm/lib/copy_template.S b/arch/arm/lib/copy_template.S
index 805e3f8..00211ce 100644
--- a/arch/arm/lib/copy_template.S
+++ b/arch/arm/lib/copy_template.S
@@ -41,22 +41,23 @@
  *	"al" condition is assumed by default.
  *
  * str1w ptr reg abort
+ * str4w ptr reg1 reg2 reg3 reg4 abort
  * str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
  * str1b ptr reg cond abort
  *
  *	Same as their ldr* counterparts, but data is stored to 'ptr' location
  *	rather than being loaded.
  *
- * enter reg1 reg2
+ * enter_no_regs reg1 reg2
  *
- *	Preserve the provided registers on the stack plus any additional
- *	data as needed by the implementation including this code. Called
- *	upon code entry.
+ *	Preserve data on the stack as needed by the implementation including
+ *      this code. Called upon code entry.
  *
- * exit reg1 reg2
+ * exit_no_regs reg1 reg2
  *
- *	Restore registers with the values previously saved with the
- *	'preserv' macro. Called upon code termination.
+ *	Exit, processing data on the stack saved with the 'enter' macro.
+ *      Called upon code termination. The lr register holds the return
+ *      address.
  *
  * LDR1W_SHIFT
  * STR1W_SHIFT
@@ -64,47 +65,460 @@
  *	Correction to be applied to the "ip" register when branching into
  *	the ldr1w or str1w instructions (some of these macros may expand to
  *	than one 32bit instruction in Thumb-2)
+ *
+ * L1_CACHE_BYTES
+ *
+ *      The cache line size used for prefetches. Preloads are performed at
+ *      L1_CACHE_BYTES aligned addresses. However, if L1_CACHE_BYTES == 64,
+ *      in the case of unaligned copies preload instructions are performed
+ *      at 32 bytes aligned addresses. The code could be modified to strictly
+ *      preload at 64 bytes aligned addresses, at the cost of increasing code
+ *      size and complexity. However, the armv7 architecture doesn't seem
+ *      to incur a big penalty for the unnecessary preload instructions.
+ *      Additionally unaligned copies are rare.
+ *
+ * PREFETCH_DISTANCE
+ *
+ *      The prefetch distance in units of L1_CACHE_BYTES used for prefetches.
+ *
+ * WRITE_ALIGN_BYTES
+ *
+ *      Write aligning is enabled if the CALGN macro expands to instructions
+ *      instead of nothing. When enabled, WRITE_ALIGN_BYTES defines the number
+ *      of bytes to align to (it must be 16 or 32).
+ *
+ * COPY_FUNCTION_MEMCPY
+ *
+ *      When COPY_FUNCTION_MEMCPY is defined, there no need to use single word
+ *      loads and stores in the alignment and tail parts for the word aligned
+ *      case. This results in a measurable speed-up for modern ARM platforms.
+ *      Additionally, write alignment is disabled when COPY_FUNCTION_MEMCPY
+ *      is not defined.
+ *
+ * COPY_FUNCTION_FROM_USER
+ *
+ *      This is defined when compiling the copy_from_user function. The write
+ *      alignment code is disabled because it is slower (the main loop will
+ *      load single words any way, and the write alignment code only
+ *      constitutes overhead).
+ *
+ * COPY_FUNCTION_TO_USER
+ *
+ *      This is defined when compiling the copy_to_user and copy_to_user_std
+ *      functions. The write alignment code is disabled because it is slower
+ *      (the main loop will write single words any way, and the write alignment
+ *      code only constitutes overhead).
+ *
  */
 
+#ifdef COPY_FUNCTION_MEMCPY
+/* The small size threshold must be >= 15 for COPY_FUNCTION_MEMCPY. */
+#define SMALL_SIZE_THRESHOLD 15
+/*
+ * For regular memcpy, we have a fast path handling up to 256 bytes for
+ * word aligned requests. Because the fast path doesn't do write aligning,
+ * on platforms that appreciate write aligning setting the threshold to a
+ * lower value than 256 bytes might have a benefit. The treshold must be
+ * greater or equal to 32 for regular memcpy.
+ */
+#define FAST_PATH_SIZE_THRESHOLD 256
+#endif
+#ifdef COPY_FUNCTION_FROM_USER
+#define SMALL_SIZE_THRESHOLD 8
+/*
+ * For copy_from_user, the fast path is unoptimal for sizes greater or
+ * equal to about 96 bytes.
+ */
+#define FAST_PATH_SIZE_THRESHOLD 95
+#define DISABLE_WRITE_ALIGNMENT
+#endif
+#ifdef COPY_FUNCTION_TO_USER
+#define SMALL_SIZE_THRESHOLD 8
+/*
+ * When copy_to_user_memcpy is enabled in the kernel configuration
+ * (CONFIG_UACCESS_WITH_MEMCPY), the assembler copy_to_user function
+ * will only be called for sizes less than 64 bytes. Ideally, the
+ * fast path threshold for copy_to_user should be 63 or higher to
+ * avoid the non-fast path code completely.
+ *
+ * Otherwise, it seems the fast path is faster or almost as fast even
+ * for larger sizes.
+ */
+#define FAST_PATH_SIZE_THRESHOLD 256
+#define DISABLE_WRITE_ALIGNMENT
+#endif
 
-		enter	r4, lr
-
+#define OPTIMIZE_WITH_FAST_PATH
+
+#ifdef OPTIMIZE_WITH_FAST_PATH
+		/*
+                 * For small aligned memcpy/copy_to_user/copy_from_user
+                 * operations, the current implementation has some
+                 * overhead. By creating a fast path for common small
+                 * aligned requests, performance is increased. This
+		 * applies to both memcpy and copy_to/from_user.
+                 */
+		cmp	r2, #SMALL_SIZE_THRESHOLD
+		/* Calculate the aligned base for preloads. */
+		bic	ip, r1, #(L1_CACHE_BYTES - 1)
+		enter_no_regs
+		pld	[ip]
+		orr	r3, r0, r1
+		ble	36f
+		cmp	r2, #FAST_PATH_SIZE_THRESHOLD
+		tstle	r3, #3
+		bne	37f
+
+		/*
+		 * At this point, we have a small-to-medium sized
+		 * (<= FAST_PATH_SIZE_THRESHOLD bytes) word-aligned request
+		 * of size greater than SMALL_SIZE_THRESHOLD.
+		 */
+#ifdef COPY_FUNCTION_MEMCPY
+		/* In the case of regular memcpy, SMALL_SIZE_THRESHOLD >= 15
+		 * which means that the number of bytes >= 16 when we get here.
+		 */
+.macro check_preload bytes_to_go
+.if \bytes_to_go >= (PREFETCH_DISTANCE * L1_CACHE_BYTES) && \
+(\bytes_to_go % L1_CACHE_BYTES) == 0
+	PLD(	pld     [r1, r5]	)
+	NO_PLD(	nop )
+.else
+		nop
+.endif
+.endm
+	PLD(	pld	[ip, #L1_CACHE_BYTES] )
+		stmdb	sp!, {r4, r5, lr}
+		bic	r3, r2, #15
+		/*
+                 * Use a heuristic to determine whether the preload
+		 * at aligned_base + 2 * L1_CACHE_BYTES will be useful.
+		 */
+	PLD(	cmp	r2, #(2 * L1_CACHE_BYTES - L1_CACHE_BYTES / 2)	)
+	PLD(	add	r5, ip, #(3 * L1_CACHE_BYTES) )
+		sub	r2, r2, r3
+	PLD(	blt	40f	)
+	PLD(	pld	[ip, #(2 * L1_CACHE_BYTES)] )
+40:		rsb	r3, r3, #256
+	PLD(	sub	r5, r5, r1		)
+		add	pc, pc, r3
+		nop
+		/* 256 bytes to go. */
+		check_preload 256
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 240 bytes go. */
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 224 bytes to go. */
+		check_preload 224
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 204 bytes go. */
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 192 bytes to go. */
+		check_preload 192
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 176 bytes go. */
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 160 bytes to go. */
+		check_preload 160
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 144 bytes go. */
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 128 bytes to go. */
+		check_preload 128
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 112 bytes go. */
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 96 bytes to go. */
+		check_preload 96
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 90 bytes to go. */
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* 64 bytes to go. */
+		check_preload 64
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		nop
+		ldmia	r1!, {r3, r4, ip, lr}
+		nop
+		stmia	r0!, {r3, r4, ip, lr}
+		/* At this point there are 16 to 31 bytes to go. */
+		tst	r2, #15
+		ldmia	r1!, {r3, r4, ip, lr}
+		cmpne	r2, #8
+		/*
+		 * If r2 == 8, we need to clear the eq flag while
+		 * making sure carry remains set.
+		 */
+		tsteq	r2, #15
+		stmia	r0!, {r3, r4, ip, lr}
+		/*
+		 * The equal is set if there are no bytes left.
+		 * The carry flag is set is there are >= 8 bytes left.
+		 */
+		beq	43f
+		ldrcs	ip, [r1], #4
+		ldrcs	r3, [r1], #4
+		strcs	ip, [r0], #4
+		strcs	r3, [r0], #4
+		tst	r2, #4
+		ldmfd	sp!, {r4, r5, lr}
+		ldrne	ip, [r1], #4
+		strne	ip, [r0], #4
+		tst	r2, #3
+		ldmeqfd	sp!, {r0}
+		moveq	pc, lr
+		b	38f
+43:		ldmfd	sp!, {r4, r5, lr}
+		ldmfd	sp!, {r0}
+		mov	pc, lr
+#else
+		/*
+                 * For copy_to_user and copy_from_user, the fast path
+		 * uses single word loads and stores, but due to the
+		 * decreased overhead this can be a big win for small
+		 * sizes which are very common.
+		 */
+32:		ldr1w	r1, r3, abort=22f
+		cmp	r2, #16
+		ldr1w	r1, ip, abort=22f
+		str1w	r0, r3, abort=22f
+		sub	r2, r2, #8
+		str1w	r0, ip, abort=22f
+		bge	32b
+		tst	r2, #4
+		beq	34f
+		ldr1w	r1, r3, abort=22f
+		str1w	r0, r3, abort=22f
+#endif
+34:		tst	r2, #3
+		bne	38f
+		exit_no_regs
+
+36:		/*
+                 * At this point, we have <= SMALL_SIZE_THRESHOLD bytes that
+		 * may not be aligned. This code is optimized for < 4 bytes
+		 * or word aligned source and destination; otherwise, branch
+		 * to the general case.
+                 */
+		cmp	r2, #4
+		blt	38f
+		tst	r3, #3
+		sub	r2, r2, #3
+		bne	35f
+		/* Word aligned source and destination, >= 4 bytes. */
+44:		ldr1w	r1, r3, abort=22f
 		subs	r2, r2, #4
+		str1w	r0, r3, abort=22f
+		bgt	44b
+		adds	r2, r2, #3
+		beq	39f
+38:		movs	r2, r2, lsl #31
+		ldr1b	r1, r3, ne, abort=22f
+		ldr1b	r1, ip, cs, abort=22f
+		str1b	r0, r3, ne, abort=22f
+		ldr1b	r1, r3, cs, abort=22f
+		str1b	r0, ip, cs, abort=22f
+		str1b	r0, r3, cs, abort=22f
+39:		exit_no_regs
+
+33:		/* Unaligned case, >= 4 bytes. */
+		ands	ip, r0, #3
+		sub	r2, r2, #4
+		bne	9f
+		ands	ip, r1, #3
+		b	10f
+
+1:		/*
+		 * Unaligned case that has been aligned to a word
+		 * boundary (src & 3) == (dst & 3).
+		 */
+		subs	r2, r2, #28
+		stmfd	sp!, {r5 - r9}
+		mov	r8, r3
+		blt	5f
+		b	45f
+
+35:		add	r2, r2, #3
+
+		/*
+		 * We get here when the fast path was not selected,
+		 * which is for unaligned requests >= 4 bytes and aligned
+		 * requests > FAST_PATH_THRESHOLD. r3 is equal to the
+		 * logical OR of the source and destination addresses,
+		 * ip holds the aligned source base address.
+		 */
+37:		tst	r3, #3
+		stmdb	sp!, {r4, lr}
+		mov	r3, ip
+	PLD(	pld	[ip, #L1_CACHE_BYTES]	)
+		bne	33b	/* Unaligned. */
+
+		subs	r2, r2, #32
+		stmfd	sp!, {r5 - r9}
+		mov	r8, r3
+45:
+#else	/* defined(OPTIMIZE_WITH_FAST_PATH) */
+		/*
+		 * This is the entry point of the original function, used
+		 * when the fast path is disabled.
+		 * ip holds the aligned source base address.
+		 */
+37:		stmdb	sp!, {r4, lr}
+
+33:		subs	r2, r2, #4
+		mov	r3, ip
 		blt	8f
 		ands	ip, r0, #3
-	PLD(	pld	[r1, #0]		)
+	PLD(	pld	[r3, #L1_CACHE_BYTES]	)
 		bne	9f
 		ands	ip, r1, #3
 		bne	10f
 
 1:		subs	r2, r2, #(28)
-		stmfd	sp!, {r5 - r8}
+		stmfd	sp!, {r5 - r9}
+		mov	r8, r3
 		blt	5f
+#endif
 
-	CALGN(	ands	ip, r0, #31		)
-	CALGN(	rsb	r3, ip, #32		)
+#ifndef DISABLE_WRITE_ALIGNMENT
+	CALGN(	ands	ip, r0, #(WRITE_ALIGN_BYTES - 1)	)
+	CALGN(	rsb	r3, ip, #WRITE_ALIGN_BYTES		)
 	CALGN(	sbcnes	r4, r3, r2		)  @ C is always set here
 	CALGN(	bcs	2f			)
+#ifdef COPY_FUNCTION_MEMCPY
+		/* For regular memcpy, use conditional multiloads/stores. */
+	CALGN(	tst     r3, #4			)
+	CALGN(	ldrne   r4, [r1], #4		)
+	CALGN(	strne   r4, [r0], #4		)
+	CALGN(	tst     r3, #8			)
+	CALGN(  ldmneia r1!, {r4-r5}		)
+	CALGN(	sub     r2, r2, r3		)
+	CALGN(	stmneia r0!, {r4-r5}		)
+.if WRITE_ALIGN_BYTES == 32
+	CALGN(	tst	r3, #16			)
+	CALGN(	ldmneia r1!, {r4-r7}		)
+	CALGN(	stmneia r0!, {r4-r7}		)
+.endif
+#else
 	CALGN(	adr	r4, 6f			)
+.if WRITE_ALIGN_BYTES == 16
+	CALGN(  add	ip, ip, #16		)
+.endif
 	CALGN(	subs	r2, r2, r3		)  @ C gets set
 	CALGN(	add	pc, r4, ip		)
+#endif
+#endif
 
-	PLD(	pld	[r1, #0]		)
-2:	PLD(	subs	r2, r2, #96		)
-	PLD(	pld	[r1, #28]		)
-	PLD(	blt	4f			)
-	PLD(	pld	[r1, #60]		)
-	PLD(	pld	[r1, #92]		)
-
-3:	PLD(	pld	[r1, #124]		)
+2:
+.if L1_CACHE_BYTES == 64
+		subs    r2, r2, #32
+		blt     30f
+.endif
+		/*
+                 * Assume a preload at aligned base + 2 * L1_CACHE_BYTES will
+		 * be useful.
+		 */
+	PLD(	pld	[r8, #(2 * L1_CACHE_BYTES)]	)
+
+	PLD(	add     r9, r1, #(PREFETCH_DISTANCE * L1_CACHE_BYTES)	)
+	PLD(	subs	r2, r2, #(PREFETCH_DISTANCE * L1_CACHE_BYTES)	)
+	PLD(	bic     r3, r9, #(L1_CACHE_BYTES - 1)			)
+	PLD(	add	r8, #(3 * L1_CACHE_BYTES)	)
+	PLD(	blt	4f				)
+	PLD(	cmp	r8, r3				)
+	PLD(	sub	r9, r3, r1			)
+		/*
+		 * "Catch-up" the early preloads (which have been performed up
+		 * to aligned base + 2 * L1_CACHE_BYTES) to the preload offset
+		 * used in the main loop.
+		 */
+	PLD(	bge	41f				)
+42:	PLD(	add	r8, r8, #L1_CACHE_BYTES		)
+	PLD(	cmp	r8, r3				)
+	PLD(	pld	[r8, #(- L1_CACHE_BYTES)]	)
+	PLD(	blt	42b				)
+41:
+
+.if L1_CACHE_BYTES == 32
+3:	PLD(	pld	[r1, r9]		)
 4:		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
 		subs	r2, r2, #32
-		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		str4w	r0, r3, r4, r5, r6, abort=20f
+		str4w   r0, r7, r8, ip, lr, abort=20f
 		bge	3b
-	PLD(	cmn	r2, #96			)
+	PLD(	cmn	r2, #(PREFETCH_DISTANCE * 32)	)
 	PLD(	bge	4b			)
+.else /* L1_CACHE_BYTES == 64 */
+3:		pld	[r1, r9]
+4:		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		subs	r2, r2, #64
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		bge	3b
+		cmn	r2, #(PREFETCH_DISTANCE * 64)
+		bge	4b
 
-5:		ands	ip, r2, #28
+30:		tst     r2, #32
+		beq	31f
+		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+31:
+.endif
+
+5:
+#ifdef COPY_FUNCTION_MEMCPY
+		/*
+		 * For regular memcpy, use conditional multiloads/stores
+		 * for the tail.
+		 */
+		tst     r2, #16
+		ldmneia r1!, {r4-r7}
+		stmneia r0!, {r4-r7}
+		tst     r2, #8
+		ldmneia r1!, {r4-r5}
+		stmneia r0!, {r4-r5}
+		tst     r2, #4
+		ldrne   r4, [r1], #4
+		strne   r4, [r0], #4
+		b	7f
+#else
+		ands	ip, r2, #28
 		rsb	ip, ip, #32
 #if LDR1W_SHIFT > 0
 		lsl	ip, ip, #LDR1W_SHIFT
@@ -141,9 +555,12 @@
 		str1w	r0, r8, abort=20f
 		str1w	r0, lr, abort=20f
 
-	CALGN(	bcs	2b			)
+#ifndef DISABLE_WRITE_ALIGNMENT
+	CALGN(	bcs	2b	)
+#endif
+#endif	/* defined(COPY_FUNCTION_MEMCPY) */
 
-7:		ldmfd	sp!, {r5 - r8}
+7:		ldmfd	sp!, {r5 - r9}
 
 8:		movs	r2, r2, lsl #31
 		ldr1b	r1, r3, ne, abort=21f
@@ -153,7 +570,8 @@
 		str1b	r0, r4, cs, abort=21f
 		str1b	r0, ip, cs, abort=21f
 
-		exit	r4, pc
+		ldmfd	sp!, {r4, lr}
+		exit_no_regs
 
 9:		rsb	ip, ip, #4
 		cmp	ip, #2
@@ -180,22 +598,43 @@
 		subs	r2, r2, #28
 		blt	14f
 
-	CALGN(	ands	ip, r0, #31		)
-	CALGN(	rsb	ip, ip, #32		)
+#ifndef DISABLE_WRITE_ALIGNMENT
+	CALGN(	ands	ip, r0, #(WRITE_ALIGN_BYTES - 1)	)
+	CALGN(	rsb	ip, ip, #WRITE_ALIGN_BYTES		)
 	CALGN(	sbcnes	r4, ip, r2		)  @ C is always set here
 	CALGN(	subcc	r2, r2, ip		)
 	CALGN(	bcc	15f			)
-
-11:		stmfd	sp!, {r5 - r9}
-
-	PLD(	pld	[r1, #0]		)
-	PLD(	subs	r2, r2, #96		)
-	PLD(	pld	[r1, #28]		)
-	PLD(	blt	13f			)
-	PLD(	pld	[r1, #60]		)
-	PLD(	pld	[r1, #92]		)
-
-12:	PLD(	pld	[r1, #124]		)
+#endif
+		/*
+		 * We need the aligned base used for early preloads,
+		 * but because this is an entry point from multiple
+		 * code paths, we don't have the value available in
+		 * all cases and so cannot match up the early preloads
+		 * with the preloads in the main loop.
+		 */
+11:		stmfd	sp!, {r5 - r10}
+
+        PLD(    add     r10, r1, #(PREFETCH_DISTANCE * L1_CACHE_BYTES)	)
+        PLD(    bic     r3, r10, #(L1_CACHE_BYTES - 1)	)
+        PLD(    subs    r2, r2, #(PREFETCH_DISTANCE * 32)	)
+        PLD(    sub     r10, r3, r1		)
+        PLD(    blt     13f                     )
+.if PREFETCH_DISTANCE >= 4
+	PLD(	pld	[r3, #(- 3 * L1_CACHE_BYTES)]	)
+.endif
+.if PREFETCH_DISTANCE >= 3
+	PLD(	pld	[r3, #(- 2 * L1_CACHE_BYTES)]	)
+.endif
+	PLD(	pld	[r3, #(- L1_CACHE_BYTES)]	)
+
+		/*
+		 * Note that when L1_CACHE_BYTES is 64, we are
+		 * prefetching every 32 bytes. Although not optimal
+		 * there doesn't seem to be big penalty for the extra
+		 * preload instructions and it prevents greater
+		 * code size and complexity.
+		 */
+12:	PLD(	pld	[r1, r10]		)
 13:		ldr4w	r1, r4, r5, r6, r7, abort=19f
 		mov	r3, lr, pull #\pull
 		subs	r2, r2, #32
@@ -217,10 +656,10 @@
 		orr	ip, ip, lr, push #\push
 		str8w	r0, r3, r4, r5, r6, r7, r8, r9, ip, , abort=19f
 		bge	12b
-	PLD(	cmn	r2, #96			)
-	PLD(	bge	13b			)
+	PLD(	cmn	r2, #(PREFETCH_DISTANCE * 32)	)
+	PLD(	bge	13b				)
 
-		ldmfd	sp!, {r5 - r9}
+		ldmfd	sp!, {r5 - r10}
 
 14:		ands	ip, r2, #28
 		beq	16f
@@ -231,8 +670,10 @@
 		orr	r3, r3, lr, push #\push
 		str1w	r0, r3, abort=21f
 		bgt	15b
+#ifndef DISABLE_WRITE_ALIGNMENT
 	CALGN(	cmp	r2, #0			)
 	CALGN(	bge	11b			)
+#endif
 
 16:		sub	r1, r1, #(\push / 8)
 		b	8b
@@ -255,13 +696,10 @@
  */
 
 	.macro	copy_abort_preamble
-19:	ldmfd	sp!, {r5 - r9}
+19:	ldmfd	sp!, {r5 - r10}
 	b	21f
-20:	ldmfd	sp!, {r5 - r8}
-21:
-	.endm
-
-	.macro	copy_abort_end
-	ldmfd	sp!, {r4, pc}
+20:	ldmfd	sp!, {r5 - r9}
+21:	ldmfd	sp!, {r4, lr}
+22:
 	.endm
 
diff --git a/arch/arm/lib/copy_to_user.S b/arch/arm/lib/copy_to_user.S
index d066df6..d5db490 100644
--- a/arch/arm/lib/copy_to_user.S
+++ b/arch/arm/lib/copy_to_user.S
@@ -12,6 +12,7 @@
 
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#include <asm/cache.h>
 
 /*
  * Prototype:
@@ -39,6 +40,7 @@
 #else
 #define STR1W_SHIFT	1
 #endif
+#define COPY_FUNCTION_TO_USER
 
 	.macro ldr1w ptr reg abort
 	W(ldr) \reg, [\ptr], #4
@@ -60,6 +62,13 @@
 	strusr	\reg, \ptr, 4, abort=\abort
 	.endm
 
+	.macro str4w ptr reg1 reg2 reg3 reg4 abort
+	str1w \ptr, \reg1, \abort
+	str1w \ptr, \reg2, \abort
+	str1w \ptr, \reg3, \abort
+	str1w \ptr, \reg4, \abort
+	.endm
+
 	.macro str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
 	str1w \ptr, \reg1, \abort
 	str1w \ptr, \reg2, \abort
@@ -75,17 +84,24 @@
 	strusr	\reg, \ptr, 1, \cond, abort=\abort
 	.endm
 
-	.macro enter reg1 reg2
-	mov	r3, #0
-	stmdb	sp!, {r0, r2, r3, \reg1, \reg2}
+	.macro enter_no_regs
+	/*
+         * Push the destination and the number of bytes
+         * onto the stack.
+         */
+	stmdb	sp!, {r0, r2}
 	.endm
 
-	.macro exit reg1 reg2
+	.macro exit_no_regs
+	/* Pop the original destination and number of bytes. */
 	add	sp, sp, #8
-	ldmfd	sp!, {r0, \reg1, \reg2}
+	/* Return 0 in r0. */
+	mov	r0, #0
+	mov	pc, lr
 	.endm
 
 	.text
+	.p2align 5
 
 ENTRY(__copy_to_user_std)
 WEAK(__copy_to_user)
@@ -98,9 +114,14 @@ ENDPROC(__copy_to_user_std)
 	.pushsection .fixup,"ax"
 	.align 0
 	copy_abort_preamble
-	ldmfd	sp!, {r1, r2, r3}
+	/*
+	 * Pop the original destination and the number of bytes
+         * off the stock.
+         */
+	ldmfd	sp!, {r1, r2}
+	/* Calculate the number of bytes not copied. */
 	sub	r0, r0, r1
 	rsb	r0, r0, r2
-	copy_abort_end
+	mov	pc, lr
 	.popsection
 
diff --git a/arch/arm/lib/memcpy.S b/arch/arm/lib/memcpy.S
index a9b9e22..07cdf5a 100644
--- a/arch/arm/lib/memcpy.S
+++ b/arch/arm/lib/memcpy.S
@@ -12,9 +12,11 @@
 
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#include <asm/cache.h>
 
 #define LDR1W_SHIFT	0
 #define STR1W_SHIFT	0
+#define COPY_FUNCTION_MEMCPY
 
 	.macro ldr1w ptr reg abort
 	W(ldr) \reg, [\ptr], #4
@@ -36,6 +38,10 @@
 	W(str) \reg, [\ptr], #4
 	.endm
 
+	.macro str4w ptr reg1 reg2 reg3 reg4 abort
+	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4}
+	.endm
+
 	.macro str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
 	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4, \reg5, \reg6, \reg7, \reg8}
 	.endm
@@ -44,15 +50,19 @@
 	str\cond\()b \reg, [\ptr], #1
 	.endm
 
-	.macro enter reg1 reg2
-	stmdb sp!, {r0, \reg1, \reg2}
+	.macro enter_no_regs
+	/* Push the destination onto the stack. */
+	stmdb sp!, {r0}
 	.endm
 
-	.macro exit reg1 reg2
-	ldmfd sp!, {r0, \reg1, \reg2}
+	.macro exit_no_regs
+	/* Return the original destination in r0. */
+	ldr r0, [sp], #4
+	mov pc, lr
 	.endm
 
 	.text
+	.p2align 5
 
 /* Prototype: void *memcpy(void *dest, const void *src, size_t n); */
 
diff --git a/arch/arm/lib/memset.S b/arch/arm/lib/memset.S
index 94b0650..98bab8d 100644
--- a/arch/arm/lib/memset.S
+++ b/arch/arm/lib/memset.S
@@ -13,22 +13,22 @@
 #include <asm/assembler.h>
 
 	.text
-	.align	5
+	.p2align 5
 
 ENTRY(memset)
 	ands	r3, r0, #3		@ 1 unaligned?
 	mov	ip, r0			@ preserve r0 as return value
 	bne	6f			@ 1
 /*
- * we know that the pointer in ip is aligned to a word boundary.
+ * we know that the pointer in r0 is aligned to a word boundary.
  */
 1:	orr	r1, r1, r1, lsl #8
+	cmp	r2, #16
 	orr	r1, r1, r1, lsl #16
 	mov	r3, r1
-	cmp	r2, #16
 	blt	4f
 
-#if ! CALGN(1)+0
+#if ! CALGN_MEMSET(1)+0
 
 /*
  * We need 2 extra registers for this loop - use r8 and the LR
@@ -59,41 +59,59 @@ ENTRY(memset)
 /*
  * This version aligns the destination pointer in order to write
  * whole cache lines at once.
+ *
+ * However, at least some architectures don't like writing 32 bytes
+ * at a time. Since it's unlikely that it will hurt performance,
+ * write 16 bytes at a time, eliminating save/restore of extra
+ * registers on the stack.
  */
 
-	stmfd	sp!, {r4-r8, lr}
-	mov	r4, r1
-	mov	r5, r1
-	mov	r6, r1
-	mov	r7, r1
+	cmp	r2, #64
+	stmfd	sp!, {r8, lr}
 	mov	r8, r1
 	mov	lr, r1
+	blt	7f
+.if MEMSET_WRITE_ALIGN_BYTES == 8
+	tst	ip, #4
+	beq	3f
 
-	cmp	r2, #96
-	tstgt	ip, #31
-	ble	3f
-
-	and	r8, ip, #31
-	rsb	r8, r8, #32
-	sub	r2, r2, r8
-	movs	r8, r8, lsl #(32 - 4)
-	stmcsia	ip!, {r4, r5, r6, r7}
-	stmmiia	ip!, {r4, r5}
-	tst	r8, #(1 << 30)
-	mov	r8, r1
+	cmp	r2, #68
+	str	r1, [ip], #4
+        sub	r2, r2, #4
+	blt     7f
+.else	/* MEMSET_WRITE_ALIGN_BYTES == 32 */
+	tst	ip, #31
+	beq	3f
+	tst     ip, #4
 	strne	r1, [ip], #4
+        subne	r2, r2, #4
+	tst     ip, #8
+	stmneia ip!, {r1, r3}
+        subne   r2, r2, #8
+	tst	ip, #16
+	stmneia ip!, {r1, r3}
+        subne   r2, r2, #16
+        stmneia ip!, {r1, r3}
+	cmp	r2, #64
+	blt	7f
+.endif
 
-3:	subs	r2, r2, #64
-	stmgeia	ip!, {r1, r3-r8, lr}
-	stmgeia	ip!, {r1, r3-r8, lr}
-	bgt	3b
-	ldmeqfd	sp!, {r4-r8, pc}
+3:	stmia	ip!, {r1, r3, r8, lr}
+	sub	r2, r2, #64
+	stmia	ip!, {r1, r3, r8, lr}
+	cmp     r2, #64
+	stmia	ip!, {r1, r3, r8, lr}
+	stmia	ip!, {r1, r3, r8, lr}
+	bge	3b
+	tst     r2, #63
+	ldmeqfd	sp!, {r8, pc}
 
-	tst	r2, #32
-	stmneia	ip!, {r1, r3-r8, lr}
+7:	tst	r2, #32
+	stmneia	ip!, {r1, r3, r8, lr}
+	stmneia	ip!, {r1, r3, r8, lr}
 	tst	r2, #16
-	stmneia	ip!, {r4-r7}
-	ldmfd	sp!, {r4-r8, lr}
+	stmneia	ip!, {r1, r3, r8, lr}
+	ldmfd	sp!, {r8, lr}
 
 #endif
 
diff --git a/arch/arm/lib/memzero.S b/arch/arm/lib/memzero.S
index 3fbdef5..5bcb3e7 100644
--- a/arch/arm/lib/memzero.S
+++ b/arch/arm/lib/memzero.S
@@ -12,7 +12,6 @@
 
 	.text
 	.align	5
-	.word	0
 /*
  * Align the pointer in r0.  r3 contains the number of bytes that we are
  * mis-aligned by, and r1 is the number of bytes.  If r1 < 4, then we
@@ -21,26 +20,30 @@
 1:	subs	r1, r1, #4		@ 1 do we have enough
 	blt	5f			@ 1 bytes to align with?
 	cmp	r3, #2			@ 1
+	add	r1, r1, r3		@ 1 (r1 = r1 - (4 - r3))
 	strltb	r2, [r0], #1		@ 1
 	strleb	r2, [r0], #1		@ 1
+	mov	r3, r2
 	strb	r2, [r0], #1		@ 1
-	add	r1, r1, r3		@ 1 (r1 = r1 - (4 - r3))
 /*
  * The pointer is now aligned and the length is adjusted.  Try doing the
  * memzero again.
  */
+	b	6f
 
+	.p2align 5
 ENTRY(__memzero)
-	mov	r2, #0			@ 1
 	ands	r3, r0, #3		@ 1 unaligned?
+	mov	r2, #0			@ 1
 	bne	1b			@ 1
 /*
  * r3 = 0, and we know that the pointer in r0 is aligned to a word boundary.
  */
-	cmp	r1, #16			@ 1 we can skip this chunk if we
-	blt	4f			@ 1 have < 16 bytes
 
-#if ! CALGN(1)+0
+#if ! CALGN_MEMSET(1)+0
+
+6:	cmp	r1, #16			@ 1 we can skip this chunk if we
+	blt	4f			@ 1 have < 16 bytes
 
 /*
  * We need an extra register for this loop - save the return address and
@@ -72,39 +75,61 @@ ENTRY(__memzero)
 /*
  * This version aligns the destination pointer in order to write
  * whole cache lines at once.
+ *
+ * However, at least some architectures don't like writing 32 bytes
+ * at a time. Since it's unlikely that it will hurt performance,
+ * write 16 bytes at a time, eliminating save/restore of extra
+ * registers on the stack.
  */
 
-	stmfd	sp!, {r4-r7, lr}
-	mov	r4, r2
-	mov	r5, r2
-	mov	r6, r2
-	mov	r7, r2
+6:	str	lr, [sp, #-4]!
+
+	cmp	r1, #64
 	mov	ip, r2
-	mov	lr, r2
-
-	cmp	r1, #96
-	andgts	ip, r0, #31
-	ble	3f
-
-	rsb	ip, ip, #32
-	sub	r1, r1, ip
-	movs	ip, ip, lsl #(32 - 4)
-	stmcsia	r0!, {r4, r5, r6, r7}
-	stmmiia	r0!, {r4, r5}
-	movs	ip, ip, lsl #2
-	strcs	r2, [r0], #4
-
-3:	subs	r1, r1, #64
-	stmgeia	r0!, {r2-r7, ip, lr}
-	stmgeia	r0!, {r2-r7, ip, lr}
-	bgt	3b
-	ldmeqfd	sp!, {r4-r7, pc}
-
-	tst	r1, #32
-	stmneia	r0!, {r2-r7, ip, lr}
+	mov     lr, r2
+	blt	7f
+
+.if MEMSET_WRITE_ALIGN_BYTES == 8
+	tst	r0, #4
+	beq	3f
+
+	cmp	r1, #68
+	str	r2, [r0], #4
+        sub	r1, r1, #4
+	blt     7f
+.else	/* MEMSET_WRITE_ALIGN_BYTES == 32 */
+	tst	r0, #31
+	beq	3f
+	tst     r0, #4
+	strne	r2, [r0], #4
+        subne	r1, r1, #4
+	tst     r0, #8
+	stmneia r0!, {r2, r3}
+        subne   r1, r1, #8
+	tst	r0, #16
+	stmneia r0!, {r2, r3}
+        subne   r1, r1, #16
+        stmneia r0!, {r2, r3}
+	cmp	r1, #64
+	blt	7f
+.endif
+
+3:	stmia	r0!, {r2, r3, ip, lr}
+	sub	r1, r1, #64
+	stmia	r0!, {r2, r3, ip, lr}
+	cmp	r1, #64
+	stmia	r0!, {r2, r3, ip, lr}
+	stmia	r0!, {r2, r3, ip, lr}
+	bge	3b
+	tst	r1, #63
+	ldmeqfd	sp!, {pc}
+
+7:	tst	r1, #32
+	stmneia	r0!, {r2, r3, ip, lr}
+	stmneia	r0!, {r2, r3, ip, lr}
 	tst	r1, #16
-	stmneia	r0!, {r4-r7}
-	ldmfd	sp!, {r4-r7, lr}
+	stmneia	r0!, {r2, r3, ip, lr}
+	ldr	lr, [sp], #4
 
 #endif
 
@@ -120,6 +145,6 @@ ENTRY(__memzero)
 	strneb	r2, [r0], #1		@ 1
 	strneb	r2, [r0], #1		@ 1
 	tst	r1, #1			@ 1 a byte left over
-	strneb	r2, [r0], #1		@ 1
+	strneb	r2, [r0]		@ 1
 	mov	pc, lr			@ 1
 ENDPROC(__memzero)
-- 
1.7.9.5

