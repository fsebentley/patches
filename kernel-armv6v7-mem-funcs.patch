From 7a4c2c5198dfddd1ad647ffd31dd65595da4b9bd Mon Sep 17 00:00:00 2001
From: Harm Hanemaaijer <fgenfb@yahoo.com>
Date: Mon, 24 Jun 2013 14:41:10 +0200
Subject: [PATCH] Optimize kernel memcpy/memset functions for ARM v6/v7

Optimize prefetch for modern ARM platforms in memcpy variants by
prefetching at 32-byte or 64-byte aligned addresses, depending on
L1_CACHE_BYTES. Additionally, the CALGN macro optional code is
enabled for the v6 and v7 ARM architectures to force write
alignment to 32 bytes in the memcpy variants (16-byte write
alignment is also possible). For regular memcpy (as opposed to
copy_from_user or copy_to_user), the alignment and tail code is
optimized, using multiple word instead of single word loads and
stores. Write alignment is disabled for copy_from_user and
copy_to_user.

For memzero and memset, write alignment is enabled for armv6 with
a seperate macro CALGN_MEMSET that regulates whether alignment
code is emitted in the memset and memzero assembler functions.
MEMSET_WRITE_ALIGNMENT_BYTES sets the number of bytes to align
to (8 for armv6). For armv7, write alignment did not show benefit
in testing for memzero/memset.

The function copy_page is also optimized with L1_CACHE_BYTES
aligned prefetches.

A new constant, PREFETCH_DISTANCE, is defined in
arch/arm/include/asm/cache.h which when multiplied with
L1_CACHE_BYTES is equal to the preload offset used for
prefetching. It defaults to 3 (96) for armv6 (L1_CACHE_BYTES ==
32), and 3 (192) for armv7 (L1_CACHE_BYTES == 64).

As a side-effect, memset now properly returns the original
destination address in r0. This may fix issues compiling ARM
kernels with gcc versions greater than 4.7.

Signed-off-by: Harm Hanemaaijer <fgenfb@yahoo.com>
---
 arch/arm/include/asm/assembler.h |   24 ++++-
 arch/arm/include/asm/cache.h     |   12 +++
 arch/arm/lib/copy_from_user.S    |    7 ++
 arch/arm/lib/copy_page.S         |   64 +++++++++----
 arch/arm/lib/copy_template.S     |  188 +++++++++++++++++++++++++++++++-------
 arch/arm/lib/copy_to_user.S      |   10 ++
 arch/arm/lib/memcpy.S            |    7 ++
 arch/arm/lib/memset.S            |   89 ++++++++++--------
 arch/arm/lib/memzero.S           |   97 ++++++++++++--------
 9 files changed, 376 insertions(+), 122 deletions(-)

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 5c8b3bf4..5f3336f 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -57,10 +57,13 @@
  */
 #if __LINUX_ARM_ARCH__ >= 5
 #define PLD(code...)	code
+#define NO_PLD(code...)
 #else
 #define PLD(code...)
+#define NO_PLD(code...) code
 #endif
 
+
 /*
  * This can be used to enable code to cacheline align the destination
  * pointer when bulk writing to memory.  Experiments on StrongARM and
@@ -69,14 +72,33 @@
  * is used).
  *
  * On Feroceon there is much to gain however, regardless of cache mode.
+ * The armv6 architecture benefits from write alignment to a 32-byte
+ * boundary. On armv7, write alignment to a 32-byte boundary increases
+ * performance for regular memcpy so it is enabled.
  */
-#ifdef CONFIG_CPU_FEROCEON
+#if defined(CONFIG_CPU_FEROCEON) || __LINUX_ARM_ARCH__ >= 6
 #define CALGN(code...) code
+#define WRITE_ALIGN_BYTES 32
 #else
 #define CALGN(code...)
 #endif
 
 /*
+ * Write alignment for memset/memzero is enabled for armv6.
+ */
+
+#if defined(CONFIG_CPU_FEROCEON) || __LINUX_ARM_ARCH__ == 6
+#define CALGN_MEMSET(code...) code
+#if defined(CONFIG_CPU_FEROCEON)
+#define MEMSET_WRITE_ALIGN_BYTES 32
+#else
+#define MEMSET_WRITE_ALIGN_BYTES 8
+#endif
+#else
+#define CALGN_MEMSET(code...)
+#endif
+
+/*
  * Enable and disable interrupts
  */
 #if __LINUX_ARM_ARCH__ >= 6
diff --git a/arch/arm/include/asm/cache.h b/arch/arm/include/asm/cache.h
index 75fe66b..01e31e0 100644
--- a/arch/arm/include/asm/cache.h
+++ b/arch/arm/include/asm/cache.h
@@ -8,6 +8,18 @@
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 /*
+ * Set the prefetch distance in units of L1_CACHE_BYTES based on the
+ * cache line size. The prefetch distance is used by the memcpy
+ * variants and copy_page.
+ */
+
+#if L1_CACHE_BYTES == 64
+#define PREFETCH_DISTANCE 3
+#else
+#define PREFETCH_DISTANCE 3
+#endif
+
+/*
  * Memory returned by kmalloc() may be used for DMA, so we must make
  * sure that all such allocations are cache aligned. Otherwise,
  * unrelated code may cause parts of the buffer to be read into the
diff --git a/arch/arm/lib/copy_from_user.S b/arch/arm/lib/copy_from_user.S
index 66a477a..1f66ea7 100644
--- a/arch/arm/lib/copy_from_user.S
+++ b/arch/arm/lib/copy_from_user.S
@@ -12,6 +12,7 @@
 
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#include <asm/cache.h>
 
 /*
  * Prototype:
@@ -39,6 +40,7 @@
 #define LDR1W_SHIFT	1
 #endif
 #define STR1W_SHIFT	0
+#define COPY_FUNCTION_FROM_USER
 
 	.macro ldr1w ptr reg abort
 	ldrusr	\reg, \ptr, 4, abort=\abort
@@ -64,6 +66,10 @@
 	W(str) \reg, [\ptr], #4
 	.endm
 
+	.macro str4w ptr reg1 reg2 reg3 reg4 abort
+	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4}
+	.endm
+
 	.macro str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
 	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4, \reg5, \reg6, \reg7, \reg8}
 	.endm
@@ -83,6 +89,7 @@
 	.endm
 
 	.text
+	.p2align 5
 
 ENTRY(__copy_from_user)
 
diff --git a/arch/arm/lib/copy_page.S b/arch/arm/lib/copy_page.S
index 6ee2f67..74bfac1 100644
--- a/arch/arm/lib/copy_page.S
+++ b/arch/arm/lib/copy_page.S
@@ -14,10 +14,22 @@
 #include <asm/asm-offsets.h>
 #include <asm/cache.h>
 
-#define COPY_COUNT (PAGE_SZ / (2 * L1_CACHE_BYTES) PLD( -1 ))
+/*
+ * Notes for Raspberry Pi:
+ * RPi does not like paired preloads in a 64-byte loop. Instead,
+ * use a 32-byte loop with one preload per loop. In addition,
+ * make sure no prefetching happens beyond the source region.
+ * The prefetch distance is set to 3 (96 bytes).
+ *
+ * This version should also be usable on architectures other than
+ * armv6 with properly defined L1_CACHE_BYTES and PREFETCH_DISTANCE
+ * (e.g. armv7 with L1_CACHE_BYTES == 64).
+ */
+
+#define COPY_COUNT (PAGE_SZ / (L1_CACHE_BYTES))
 
 		.text
-		.align	5
+		.p2align	5
 /*
  * StrongARM optimised copy_page routine
  * now 1.78bytes/cycle, was 1.60 bytes/cycle (50MHz bus -> 89MB/s)
@@ -25,23 +37,41 @@
  * the core clock switching.
  */
 ENTRY(copy_page)
-		stmfd	sp!, {r4, lr}			@	2
+		stmfd	sp!, {r4-r8, lr}		@	2
 	PLD(	pld	[r1, #0]		)
 	PLD(	pld	[r1, #L1_CACHE_BYTES]		)
+.if PREFETCH_DISTANCE > 2
+	PLD(	pld	[r1, #2 * L1_CACHE_BYTES]	)
+.if PREFETCH_DISTANCE > 3
+	PLD(	pld	[r1, #3 * L1_CACHE_BYTES]	)
+.if PREFETCH_DISTANCE > 4
+	PLD(	pld	[r1, #4 * L1_CACHE_BYTES]	)
+.endif
+.endif
+.endif
 		mov	r2, #COPY_COUNT			@	1
-		ldmia	r1!, {r3, r4, ip, lr}		@	4+1
-1:	PLD(	pld	[r1, #2 * L1_CACHE_BYTES])
-	PLD(	pld	[r1, #3 * L1_CACHE_BYTES])
+1:	PLD(	pld	[r1, #PREFETCH_DISTANCE * L1_CACHE_BYTES])
 2:
-	.rept	(2 * L1_CACHE_BYTES / 16 - 1)
-		stmia	r0!, {r3, r4, ip, lr}		@	4
-		ldmia	r1!, {r3, r4, ip, lr}		@	4
-	.endr
-		subs	r2, r2, #1			@	1
-		stmia	r0!, {r3, r4, ip, lr}		@	4
-		ldmgtia	r1!, {r3, r4, ip, lr}		@	4
-		bgt	1b				@	1
-	PLD(	ldmeqia r1!, {r3, r4, ip, lr}	)
-	PLD(	beq	2b			)
-		ldmfd	sp!, {r4, pc}			@	3
+.if L1_CACHE_BYTES == 32
+		ldmia	r1!, {r3-r6}			@	4+1
+	PLD(	sub	r2, r2, #1		)	@	1
+	NO_PLD(	subs	r2, r2, #1		)	@	1
+		ldmia   r1!, {r7, r8, ip, lr}
+		stmia	r0!, {r3-r6}			@	4
+	PLD(	cmp	r2, #PREFETCH_DISTANCE	)
+		stmia   r0!, {r7, r8, ip, lr}
+.else /* L1_CACHE_BYTES == 64 */
+		ldmia   r1!, {r3-r8, ip, lr}
+	PLD(	sub	r2, r2, #1		)	@	1
+	NO_PLD(	subs	r2, r2, #1		)	@	1
+		stmia	r0!, {r3-r8, ip, lr}		@	4
+		ldmia   r1!, {r3-r8, ip, lr}
+	PLD(	cmp	r2, #PREFETCH_DISTANCE	)
+		stmia	r0!, {r3-r8, ip, lr}		@	4
+.endif
+	PLD(	bgt	1b			)	@	1
+	NO_PLD(	bne	1b			)
+	PLD(	teq	r2, #0			)
+	PLD(	bne	2b			)
+		ldmfd	sp!, {r4-r8, pc}		@	3
 ENDPROC(copy_page)
diff --git a/arch/arm/lib/copy_template.S b/arch/arm/lib/copy_template.S
index 805e3f8..ebb76d4 100644
--- a/arch/arm/lib/copy_template.S
+++ b/arch/arm/lib/copy_template.S
@@ -64,47 +64,158 @@
  *	Correction to be applied to the "ip" register when branching into
  *	the ldr1w or str1w instructions (some of these macros may expand to
  *	than one 32bit instruction in Thumb-2)
+ *
+ * L1_CACHE_BYTES
+ *
+ *      The cache line size used for prefetches. Preloads are performed at
+ *      L1_CACHE_BYTES aligned addresses. However, if L1_CACHE_BYTES == 64,
+ *      in the case of unaligned copies preload instructions are performed
+ *      at 32 bytes aligned addresses. The code could be modified to strictly
+ *      preload at 64 bytes aligned addresses, at the cost of increasing code
+ *      size and complexity. However, the armv7 architecture doesn't seem
+ *      to incur a big penalty for the unnecessary preload instructions.
+ *      Additionally unaligned copies are rare.
+ *
+ * PREFETCH_DISTANCE
+ *
+ *      The prefetch distance in units of L1_CACHE_BYTES used for prefetches.
+ *
+ * WRITE_ALIGN_BYTES
+ *
+ *      Write aligning is enabled if the CALGN macro expands to instructions
+ *      instead of nothing. When enabled, WRITE_ALIGN_BYTES defines the number
+ *      of bytes to align to (it must be 16 or 32).
+ *
+ * COPY_FUNCTION_MEMCPY
+ *
+ *      When COPY_FUNCTION_MEMCPY is defined, there no need to use single word
+ *      loads and stores in the alignment and tail parts for the word aligned
+ *      case. This results in a measurable speed-up for modern ARM platforms.
+ *      Additionally, write alignment is disabled when COPY_FUNCTION_MEMCPY
+ *      is not defined.
+ *
+ * COPY_FUNCTION_FROM_USER
+ *
+ *      This is defined when compiling the copy_from_user function. The write
+ *      alignment code is disabled because it is slower (the main loop will
+ *      load single words any way, and the write alignment code only
+ *      constitutes overhead).
+ *
+ * COPY_FUNCTION_TO_USER
+ *
+ *      This is defined when compiling the copy_to_user and copy_to_user_std
+ *      functions. The write alignment code is disabled because it is slower
+ *      (the main loop will write single words any way, and the write alignment
+ *      code only constitutes overhead).
+ *
  */
 
+#ifndef COPY_FUNCTION_MEMCPY
+#define DISABLE_WRITE_ALIGNMENT
+#endif
 
 		enter	r4, lr
 
 		subs	r2, r2, #4
+		bic     r3, r1, #(L1_CACHE_BYTES - 1)
 		blt	8f
 		ands	ip, r0, #3
-	PLD(	pld	[r1, #0]		)
+	PLD(	pld	[r3]			)
 		bne	9f
 		ands	ip, r1, #3
 		bne	10f
 
 1:		subs	r2, r2, #(28)
-		stmfd	sp!, {r5 - r8}
+		stmfd	sp!, {r5 - r9}
 		blt	5f
 
-	CALGN(	ands	ip, r0, #31		)
-	CALGN(	rsb	r3, ip, #32		)
+#ifndef DISABLE_WRITE_ALIGNMENT
+	CALGN(	ands	ip, r0, #(WRITE_ALIGN_BYTES - 1)	)
+	CALGN(	rsb	r3, ip, #WRITE_ALIGN_BYTES		)
 	CALGN(	sbcnes	r4, r3, r2		)  @ C is always set here
 	CALGN(	bcs	2f			)
+#ifdef REGULAR_MEMCPY
+	CALGN(	tst     r3, #4			)
+	CALGN(	ldrne   r4, [r1], #4		)
+	CALGN(	strne   r4, [r0], #4		)
+	CALGN(	tst     r3, #8			)
+	CALGN(  ldmneia r1!, {r4-r5}		)
+	CALGN(	sub     r2, r2, r3		)
+	CALGN(	stmneia r0!, {r4-r5}		)
+.if WRITE_ALIGN_BYTES == 32
+	CALGN(	tst	r3, #16			)
+	CALGN(	ldmneia r1!, {r4-r7}		)
+	CALGN(	stmneia r0!, {r4-r7}		)
+.endif
+#else
 	CALGN(	adr	r4, 6f			)
+.if WRITE_ALIGN_BYTES == 16
+	CALGN(  add	ip, ip, #16		)
+.endif
 	CALGN(	subs	r2, r2, r3		)  @ C gets set
 	CALGN(	add	pc, r4, ip		)
+#endif
+#endif
 
-	PLD(	pld	[r1, #0]		)
-2:	PLD(	subs	r2, r2, #96		)
-	PLD(	pld	[r1, #28]		)
+2:
+.if L1_CACHE_BYTES == 64
+		subs    r2, r2, #32
+		blt     30f
+.endif
+	PLD(	add     r9, r1, #(PREFETCH_DISTANCE * L1_CACHE_BYTES)	)
+	PLD(	bic     r3, r9, #(L1_CACHE_BYTES - 1)			)
+	PLD(	subs	r2, r2, #(PREFETCH_DISTANCE * L1_CACHE_BYTES)	)
+	PLD(	sub	r9, r3, r1		)
 	PLD(	blt	4f			)
-	PLD(	pld	[r1, #60]		)
-	PLD(	pld	[r1, #92]		)
-
-3:	PLD(	pld	[r1, #124]		)
+.if PREFETCH_DISTANCE >= 4
+	PLD(	pld	[r3, #(- 3 * L1_CACHE_BYTES)]	)
+.endif
+.if PREFETCH_DISTANCE >= 3
+	PLD(	pld	[r3, #(- 2 * L1_CACHE_BYTES)]	)
+.endif
+	PLD(	pld	[r3, #(- L1_CACHE_BYTES)]	)
+
+.if L1_CACHE_BYTES == 32
+3:	PLD(	pld	[r1, r9]		)
 4:		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
 		subs	r2, r2, #32
-		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		str4w	r0, r3, r4, r5, r6, abort=20f
+		str4w   r0, r7, r8, ip, lr, abort=20f
 		bge	3b
-	PLD(	cmn	r2, #96			)
+	PLD(	cmn	r2, #(PREFETCH_DISTANCE * 32)	)
 	PLD(	bge	4b			)
+.else /* L1_CACHE_BYTES == 64 */
+3:		pld	[r1, r9]
+4:		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		subs	r2, r2, #64
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		bge	3b
+		cmn	r2, #(PREFETCH_DISTANCE * 64)
+		bge	4b
 
-5:		ands	ip, r2, #28
+30:		tst     r2, #32
+		beq	31f
+		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+31:
+.endif
+
+5:
+#ifdef REGULAR_MEMCPY
+		tst     r2, #16
+		ldmneia r1!, {r4-r7}
+		stmneia r0!, {r4-r7}
+		tst     r2, #8
+		ldmneia r1!, {r4-r5}
+		stmneia r0!, {r4-r5}
+		tst     r2, #4
+		ldrne   r4, [r1], #4
+		strne   r4, [r0], #4
+		b	7f
+#else
+		ands	ip, r2, #28
 		rsb	ip, ip, #32
 #if LDR1W_SHIFT > 0
 		lsl	ip, ip, #LDR1W_SHIFT
@@ -141,9 +252,12 @@
 		str1w	r0, r8, abort=20f
 		str1w	r0, lr, abort=20f
 
-	CALGN(	bcs	2b			)
+#ifndef DISABLE_WRITE_ALIGNMENT
+	CALGN(	bcs	2b	)
+#endif
+#endif	/* defined(REGULAR_MEMCPY) */
 
-7:		ldmfd	sp!, {r5 - r8}
+7:		ldmfd	sp!, {r5 - r9}
 
 8:		movs	r2, r2, lsl #31
 		ldr1b	r1, r3, ne, abort=21f
@@ -180,22 +294,30 @@
 		subs	r2, r2, #28
 		blt	14f
 
-	CALGN(	ands	ip, r0, #31		)
-	CALGN(	rsb	ip, ip, #32		)
+#ifndef DISABLE_WRITE_ALIGNMENT
+	CALGN(	ands	ip, r0, #(WRITE_ALIGN_BYTES - 1)	)
+	CALGN(	rsb	ip, ip, #WRITE_ALIGN_BYTES		)
 	CALGN(	sbcnes	r4, ip, r2		)  @ C is always set here
 	CALGN(	subcc	r2, r2, ip		)
 	CALGN(	bcc	15f			)
+#endif
 
-11:		stmfd	sp!, {r5 - r9}
-
-	PLD(	pld	[r1, #0]		)
-	PLD(	subs	r2, r2, #96		)
-	PLD(	pld	[r1, #28]		)
-	PLD(	blt	13f			)
-	PLD(	pld	[r1, #60]		)
-	PLD(	pld	[r1, #92]		)
-
-12:	PLD(	pld	[r1, #124]		)
+11:		stmfd	sp!, {r5 - r10}
+
+        PLD(    add     r10, r1, #(PREFETCH_DISTANCE * L1_CACHE_BYTES)	)
+        PLD(    bic     r3, r10, #(L1_CACHE_BYTES - 1)	)
+        PLD(    subs    r2, r2, #(PREFETCH_DISTANCE * 32)	)
+        PLD(    sub     r10, r3, r1		)
+        PLD(    blt     13f                     )
+.if PREFETCH_DISTANCE >= 4
+	PLD(	pld	[r3, #(- 3 * L1_CACHE_BYTES)]	)
+.endif
+.if PREFETCH_DISTANCE >= 3
+	PLD(	pld	[r3, #(- 2 * L1_CACHE_BYTES)]	)
+.endif
+	PLD(	pld	[r3, #(- L1_CACHE_BYTES)]	)
+
+12:	PLD(	pld	[r1, r10]		)
 13:		ldr4w	r1, r4, r5, r6, r7, abort=19f
 		mov	r3, lr, pull #\pull
 		subs	r2, r2, #32
@@ -217,10 +339,10 @@
 		orr	ip, ip, lr, push #\push
 		str8w	r0, r3, r4, r5, r6, r7, r8, r9, ip, , abort=19f
 		bge	12b
-	PLD(	cmn	r2, #96			)
-	PLD(	bge	13b			)
+	PLD(	cmn	r2, #(PREFETCH_DISTANCE * 32)	)
+	PLD(	bge	13b				)
 
-		ldmfd	sp!, {r5 - r9}
+		ldmfd	sp!, {r5 - r10}
 
 14:		ands	ip, r2, #28
 		beq	16f
@@ -231,8 +353,10 @@
 		orr	r3, r3, lr, push #\push
 		str1w	r0, r3, abort=21f
 		bgt	15b
+#ifndef DISABLE_WRITE_ALIGNMENT
 	CALGN(	cmp	r2, #0			)
 	CALGN(	bge	11b			)
+#endif
 
 16:		sub	r1, r1, #(\push / 8)
 		b	8b
@@ -257,7 +381,7 @@
 	.macro	copy_abort_preamble
 19:	ldmfd	sp!, {r5 - r9}
 	b	21f
-20:	ldmfd	sp!, {r5 - r8}
+20:	ldmfd	sp!, {r5 - r9}
 21:
 	.endm
 
diff --git a/arch/arm/lib/copy_to_user.S b/arch/arm/lib/copy_to_user.S
index d066df6..b1b8058 100644
--- a/arch/arm/lib/copy_to_user.S
+++ b/arch/arm/lib/copy_to_user.S
@@ -12,6 +12,7 @@
 
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#include <asm/cache.h>
 
 /*
  * Prototype:
@@ -39,6 +40,7 @@
 #else
 #define STR1W_SHIFT	1
 #endif
+#define COPY_FUNCTION_TO_USER
 
 	.macro ldr1w ptr reg abort
 	W(ldr) \reg, [\ptr], #4
@@ -60,6 +62,13 @@
 	strusr	\reg, \ptr, 4, abort=\abort
 	.endm
 
+	.macro str4w ptr reg1 reg2 reg3 reg4 abort
+	str1w \ptr, \reg1, \abort
+	str1w \ptr, \reg2, \abort
+	str1w \ptr, \reg3, \abort
+	str1w \ptr, \reg4, \abort
+	.endm
+
 	.macro str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
 	str1w \ptr, \reg1, \abort
 	str1w \ptr, \reg2, \abort
@@ -86,6 +95,7 @@
 	.endm
 
 	.text
+	.p2align 5
 
 ENTRY(__copy_to_user_std)
 WEAK(__copy_to_user)
diff --git a/arch/arm/lib/memcpy.S b/arch/arm/lib/memcpy.S
index a9b9e22..82476ca 100644
--- a/arch/arm/lib/memcpy.S
+++ b/arch/arm/lib/memcpy.S
@@ -12,9 +12,11 @@
 
 #include <linux/linkage.h>
 #include <asm/assembler.h>
+#include <asm/cache.h>
 
 #define LDR1W_SHIFT	0
 #define STR1W_SHIFT	0
+#define COPY_FUNCTION_MEMCPY
 
 	.macro ldr1w ptr reg abort
 	W(ldr) \reg, [\ptr], #4
@@ -36,6 +38,10 @@
 	W(str) \reg, [\ptr], #4
 	.endm
 
+	.macro str4w ptr reg1 reg2 reg3 reg4 abort
+	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4}
+	.endm
+
 	.macro str8w ptr reg1 reg2 reg3 reg4 reg5 reg6 reg7 reg8 abort
 	stmia \ptr!, {\reg1, \reg2, \reg3, \reg4, \reg5, \reg6, \reg7, \reg8}
 	.endm
@@ -53,6 +59,7 @@
 	.endm
 
 	.text
+	.p2align 5
 
 /* Prototype: void *memcpy(void *dest, const void *src, size_t n); */
 
diff --git a/arch/arm/lib/memset.S b/arch/arm/lib/memset.S
index 650d592..9aa7b26 100644
--- a/arch/arm/lib/memset.S
+++ b/arch/arm/lib/memset.S
@@ -13,8 +13,7 @@
 #include <asm/assembler.h>
 
 	.text
-	.align	5
-	.word	0
+	.p2align 5
 
 1:	subs	r2, r2, #4		@ 1 do we have enough
 	blt	5f			@ 1 bytes to align with?
@@ -23,30 +22,33 @@
 	strleb	r1, [r0], #1		@ 1
 	strb	r1, [r0], #1		@ 1
 	add	r2, r2, r3		@ 1 (r2 = r2 - (4 - r3))
+	b	6f
 /*
  * The pointer is now aligned and the length is adjusted.  Try doing the
  * memset again.
+ * Note: There is no need to recheck for alignment; by branching over the
+ * alignment check we can push r0 and lr in the entry code.
  */
 
 ENTRY(memset)
 	ands	r3, r0, #3		@ 1 unaligned?
+	stmfd	sp!, {r0, lr}
 	bne	1b			@ 1
 /*
  * we know that the pointer in r0 is aligned to a word boundary.
  */
-	orr	r1, r1, r1, lsl #8
+6:	orr	r1, r1, r1, lsl #8
+	cmp	r2, #16
 	orr	r1, r1, r1, lsl #16
 	mov	r3, r1
-	cmp	r2, #16
 	blt	4f
 
-#if ! CALGN(1)+0
+#if ! CALGN_MEMSET(1)+0
 
 /*
  * We need an extra register for this loop - save the return address and
  * use the LR
  */
-	str	lr, [sp, #-4]!
 	mov	ip, r1
 	mov	lr, r1
 
@@ -56,7 +58,7 @@ ENTRY(memset)
 	stmgeia	r0!, {r1, r3, ip, lr}
 	stmgeia	r0!, {r1, r3, ip, lr}
 	bgt	2b
-	ldmeqfd	sp!, {pc}		@ Now <64 bytes to go.
+	ldmeqfd	sp!, {r0, pc}		@ Now <64 bytes to go.
 /*
  * No need to correct the count; we're only testing bits from now on
  */
@@ -65,48 +67,63 @@ ENTRY(memset)
 	stmneia	r0!, {r1, r3, ip, lr}
 	tst	r2, #16
 	stmneia	r0!, {r1, r3, ip, lr}
-	ldr	lr, [sp], #4
 
 #else
 
 /*
  * This version aligns the destination pointer in order to write
  * whole cache lines at once.
+ *
+ * However, at least some architectures don't like writing 32 bytes
+ * at a time. Since it's unlikely that it will hurt performance,
+ * write 16 bytes at a time, eliminating save/restore of extra
+ * registers on the stack.
  */
 
-	stmfd	sp!, {r4-r7, lr}
-	mov	r4, r1
-	mov	r5, r1
-	mov	r6, r1
-	mov	r7, r1
+	cmp	r2, #64
 	mov	ip, r1
 	mov	lr, r1
+	blt	7f
+.if MEMSET_WRITE_ALIGN_BYTES == 8
+	tst	r0, #4
+	beq	3f
 
-	cmp	r2, #96
-	tstgt	r0, #31
-	ble	3f
-
-	and	ip, r0, #31
-	rsb	ip, ip, #32
-	sub	r2, r2, ip
-	movs	ip, ip, lsl #(32 - 4)
-	stmcsia	r0!, {r4, r5, r6, r7}
-	stmmiia	r0!, {r4, r5}
-	tst	ip, #(1 << 30)
-	mov	ip, r1
+	cmp	r2, #68
+	str	r1, [r0], #4
+        sub	r2, r2, #4
+	blt     7f
+.else	/* MEMSET_WRITE_ALIGN_BYTES == 32 */
+	tst	r0, #31
+	beq	3f
+	tst     r0, #4
 	strne	r1, [r0], #4
+        subne	r2, r2, #4
+	tst     r0, #8
+	stmneia r0!, {r1, r3}
+        subne   r2, r2, #8
+	tst	r0, #16
+	stmneia r0!, {r1, r3}
+        subne   r2, r2, #16
+        stmneia r0!, {r1, r3}
+	cmp	r2, #64
+	blt	7f
+.endif
 
-3:	subs	r2, r2, #64
-	stmgeia	r0!, {r1, r3-r7, ip, lr}
-	stmgeia	r0!, {r1, r3-r7, ip, lr}
-	bgt	3b
-	ldmeqfd	sp!, {r4-r7, pc}
+3:	stmia	r0!, {r1, r3, ip, lr}
+	sub	r2, r2, #64
+	stmia	r0!, {r1, r3, ip, lr}
+	cmp     r2, #64
+	stmia	r0!, {r1, r3, ip, lr}
+	stmia	r0!, {r1, r3, ip, lr}
+	bge	3b
+	tst     r2, #63
+	ldmeqfd	sp!, {r0, pc}
 
-	tst	r2, #32
-	stmneia	r0!, {r1, r3-r7, ip, lr}
+7:	tst	r2, #32
+	stmneia	r0!, {r1, r3, ip, lr}
+	stmneia	r0!, {r1, r3, ip, lr}
 	tst	r2, #16
-	stmneia	r0!, {r4-r7}
-	ldmfd	sp!, {r4-r7, lr}
+	stmneia	r0!, {r1, r3, ip, lr}
 
 #endif
 
@@ -122,6 +139,6 @@ ENTRY(memset)
 	strneb	r1, [r0], #1
 	strneb	r1, [r0], #1
 	tst	r2, #1
-	strneb	r1, [r0], #1
-	mov	pc, lr
+	strneb	r1, [r0]
+	ldmfd	sp!, {r0, pc}
 ENDPROC(memset)
diff --git a/arch/arm/lib/memzero.S b/arch/arm/lib/memzero.S
index 3fbdef5..5bcb3e7 100644
--- a/arch/arm/lib/memzero.S
+++ b/arch/arm/lib/memzero.S
@@ -12,7 +12,6 @@
 
 	.text
 	.align	5
-	.word	0
 /*
  * Align the pointer in r0.  r3 contains the number of bytes that we are
  * mis-aligned by, and r1 is the number of bytes.  If r1 < 4, then we
@@ -21,26 +20,30 @@
 1:	subs	r1, r1, #4		@ 1 do we have enough
 	blt	5f			@ 1 bytes to align with?
 	cmp	r3, #2			@ 1
+	add	r1, r1, r3		@ 1 (r1 = r1 - (4 - r3))
 	strltb	r2, [r0], #1		@ 1
 	strleb	r2, [r0], #1		@ 1
+	mov	r3, r2
 	strb	r2, [r0], #1		@ 1
-	add	r1, r1, r3		@ 1 (r1 = r1 - (4 - r3))
 /*
  * The pointer is now aligned and the length is adjusted.  Try doing the
  * memzero again.
  */
+	b	6f
 
+	.p2align 5
 ENTRY(__memzero)
-	mov	r2, #0			@ 1
 	ands	r3, r0, #3		@ 1 unaligned?
+	mov	r2, #0			@ 1
 	bne	1b			@ 1
 /*
  * r3 = 0, and we know that the pointer in r0 is aligned to a word boundary.
  */
-	cmp	r1, #16			@ 1 we can skip this chunk if we
-	blt	4f			@ 1 have < 16 bytes
 
-#if ! CALGN(1)+0
+#if ! CALGN_MEMSET(1)+0
+
+6:	cmp	r1, #16			@ 1 we can skip this chunk if we
+	blt	4f			@ 1 have < 16 bytes
 
 /*
  * We need an extra register for this loop - save the return address and
@@ -72,39 +75,61 @@ ENTRY(__memzero)
 /*
  * This version aligns the destination pointer in order to write
  * whole cache lines at once.
+ *
+ * However, at least some architectures don't like writing 32 bytes
+ * at a time. Since it's unlikely that it will hurt performance,
+ * write 16 bytes at a time, eliminating save/restore of extra
+ * registers on the stack.
  */
 
-	stmfd	sp!, {r4-r7, lr}
-	mov	r4, r2
-	mov	r5, r2
-	mov	r6, r2
-	mov	r7, r2
+6:	str	lr, [sp, #-4]!
+
+	cmp	r1, #64
 	mov	ip, r2
-	mov	lr, r2
-
-	cmp	r1, #96
-	andgts	ip, r0, #31
-	ble	3f
-
-	rsb	ip, ip, #32
-	sub	r1, r1, ip
-	movs	ip, ip, lsl #(32 - 4)
-	stmcsia	r0!, {r4, r5, r6, r7}
-	stmmiia	r0!, {r4, r5}
-	movs	ip, ip, lsl #2
-	strcs	r2, [r0], #4
-
-3:	subs	r1, r1, #64
-	stmgeia	r0!, {r2-r7, ip, lr}
-	stmgeia	r0!, {r2-r7, ip, lr}
-	bgt	3b
-	ldmeqfd	sp!, {r4-r7, pc}
-
-	tst	r1, #32
-	stmneia	r0!, {r2-r7, ip, lr}
+	mov     lr, r2
+	blt	7f
+
+.if MEMSET_WRITE_ALIGN_BYTES == 8
+	tst	r0, #4
+	beq	3f
+
+	cmp	r1, #68
+	str	r2, [r0], #4
+        sub	r1, r1, #4
+	blt     7f
+.else	/* MEMSET_WRITE_ALIGN_BYTES == 32 */
+	tst	r0, #31
+	beq	3f
+	tst     r0, #4
+	strne	r2, [r0], #4
+        subne	r1, r1, #4
+	tst     r0, #8
+	stmneia r0!, {r2, r3}
+        subne   r1, r1, #8
+	tst	r0, #16
+	stmneia r0!, {r2, r3}
+        subne   r1, r1, #16
+        stmneia r0!, {r2, r3}
+	cmp	r1, #64
+	blt	7f
+.endif
+
+3:	stmia	r0!, {r2, r3, ip, lr}
+	sub	r1, r1, #64
+	stmia	r0!, {r2, r3, ip, lr}
+	cmp	r1, #64
+	stmia	r0!, {r2, r3, ip, lr}
+	stmia	r0!, {r2, r3, ip, lr}
+	bge	3b
+	tst	r1, #63
+	ldmeqfd	sp!, {pc}
+
+7:	tst	r1, #32
+	stmneia	r0!, {r2, r3, ip, lr}
+	stmneia	r0!, {r2, r3, ip, lr}
 	tst	r1, #16
-	stmneia	r0!, {r4-r7}
-	ldmfd	sp!, {r4-r7, lr}
+	stmneia	r0!, {r2, r3, ip, lr}
+	ldr	lr, [sp], #4
 
 #endif
 
@@ -120,6 +145,6 @@ ENTRY(__memzero)
 	strneb	r2, [r0], #1		@ 1
 	strneb	r2, [r0], #1		@ 1
 	tst	r1, #1			@ 1 a byte left over
-	strneb	r2, [r0], #1		@ 1
+	strneb	r2, [r0]		@ 1
 	mov	pc, lr			@ 1
 ENDPROC(__memzero)
-- 
1.7.9.5

